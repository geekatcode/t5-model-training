{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Training T5 Model ======================#\n",
    "# ==============================================================#\n",
    "\n",
    "import sys, os\n",
    "from os import environ\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from logging.config import fileConfig\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, load_metric, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ==============================================================#\n",
    "# ====================== GLOBAL VARIABLES ======================#\n",
    "\n",
    "GlobalVar           = 0\n",
    "train_batch_size    = 8\n",
    "eval_batch_size     = 8\n",
    "max_input_length    = 512\n",
    "max_target_length   = 64\n",
    "\n",
    "MODEL_DIR           = \"/Users/sree/.cache/huggingface/hub/models--mrm8488--t5-base-finetuned-wikiSQL/\"\n",
    "MODEL_NAME          = \"mrm8488/t5-base-finetuned-wikiSQL\"\n",
    "\n",
    "DS_LOCAL            = 'my_sql_data.json'\n",
    "DS_HUGGINGFACE      = 'wikisql'\n",
    "USE_LOCAL_DATASET   = False\n",
    "DATASET             = DS_LOCAL if USE_LOCAL_DATASET else DS_HUGGINGFACE\n",
    "DATA_SAMPLING       = True\n",
    "SAMPLING_SPLIT      = {'test':50, 'train':20, 'validation':20}\n",
    "\n",
    "DATASET_PATH         = \"../data/dataset/\"\n",
    "ENCODED_DATASET_PATH = \"../data/encoded-\" + DATASET\n",
    "ENCODED_DATASET_PATH += \"-sample\" if DATA_SAMPLING else \"\"\n",
    "TASK_PREFIX         = \"translate English to SQL : \"\n",
    "\n",
    "# use_fast=True param to speed up tokenization\n",
    "# initialize model & it's tokenizer\n",
    "tokenizer           = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model               = AutoModelWithLMHead.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# initialize optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)\n",
    "\n",
    "training_args       = TrainingArguments(output_dir='./results')\n",
    "\n",
    "# ==============================================================#\n",
    "# ====================== GLOBAL FUNCTIONS ======================#\n",
    "\n",
    "def compute_metric(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def get_training_args():\n",
    "    \"\"\"\n",
    "    seq_training_args = Seq2SeqTrainingArguments(\n",
    "        MODEL_DIR,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        learning_rate=4e-5,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=1,\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\"\n",
    "        #fp16=True, # can be used with CUDA devices, not CPU\n",
    "        #report_to=\"tensorboard\" #still to make it work\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='../results',\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps = 1000,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"rouge1\"\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "def get_trainer(tokenized_dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    seq_trainer = Seq2SeqTrainer(\n",
    "        model_init      = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = tokenized_dataset[\"train\"],\n",
    "        eval_dataset    = tokenized_dataset[\"validation\"],\n",
    "        data_collator   = data_collator,\n",
    "        tokenizer       = tokenizer,\n",
    "        compute_metrics = compute_metric\n",
    "    )\n",
    "    \"\"\"\n",
    "    trainer = Trainer(\n",
    "        model           = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = tokenized_dataset[\"train\"],\n",
    "        eval_dataset    = tokenized_dataset[\"validation\"],\n",
    "        data_collator   = data_collator, #extra\n",
    "        tokenizer       = tokenizer, #extra\n",
    "        compute_metrics = compute_metric #extra\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def load_data():\n",
    "    my_ds = load_dataset(\n",
    "        'json', data_files = DATASET_PATH + DATASET) if USE_LOCAL_DATASET else load_dataset(DATASET)\n",
    "    \n",
    "    # Applicable for HF dataset only, loading samples for fast testing\n",
    "    if( not USE_LOCAL_DATASET and DATA_SAMPLING):\n",
    "        # shuffle(seed=20) for randomness\n",
    "        my_ds = DatasetDict({\n",
    "            \"test\":my_ds[\"test\"].shuffle(seed=20).select(range(SAMPLING_SPLIT['test'])),\n",
    "            \"train\": my_ds[\"train\"].shuffle(seed=20).select(range(SAMPLING_SPLIT['train'])), \n",
    "            \"validation\": my_ds[\"validation\"].shuffle(seed=20).select(range(SAMPLING_SPLIT['validation']))\n",
    "            })\n",
    "        \n",
    "    #train_dataset = load_dataset('wikisql', split=datasets.Split.TRAIN)\n",
    "    #valid_dataset = load_dataset('wikisql', split=datasets.Split.VALIDATION)\n",
    "    \"\"\"\n",
    "    DatasetDict({\n",
    "        test: Dataset({\n",
    "            features: ['phase', 'question', 'table', 'sql'],\n",
    "            num_rows: 15878\n",
    "        })\n",
    "        validation: Dataset({\n",
    "            features: ['phase', 'question', 'table', 'sql'],\n",
    "            num_rows: 8421\n",
    "        })\n",
    "        train: Dataset({\n",
    "            features: ['phase', 'question', 'table', 'sql'],\n",
    "            num_rows: 56355\n",
    "        })\n",
    "    })\n",
    "    \"\"\"\n",
    "    return my_ds\n",
    "\n",
    "def check_interrupt():\n",
    "    global GlobalVar\n",
    "    if GlobalVar > 0:\n",
    "        if GlobalVar > 5:\n",
    "            sys.exit()\n",
    "        GlobalVar += 1\n",
    "\n",
    "def preprocess_data(record):\n",
    "    \n",
    "    #check_interrupt()\n",
    "    #print(\"\\nQUE == \" , record['question'])\n",
    "    #print(\"\\nSQL == \" , record['sql'])\n",
    "    \n",
    "    input_question = []\n",
    "    expected_sqls = []\n",
    "    input_question += [TASK_PREFIX + q for q in record['question']] \n",
    "    expected_sqls += [q['human_readable'] for q in record['sql']]\n",
    "\n",
    "    print(input_question)\n",
    "    print(expected_sqls)\n",
    "    \n",
    "    model_inputs = tokenizer(input_question, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    #with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(expected_sqls, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    #Return a dictionary containing the token ids and attention masks of the inputs, and only token ids of the labels\n",
    "    return model_inputs\n",
    "\n",
    "def tokenize_dataset(dataset_to_tokenize):\n",
    "    \n",
    "    if not os.path.exists(ENCODED_DATASET_PATH):\n",
    "        print(\"Tokenizing dataset ... it will take some time\")\n",
    "        tokenized_dataset = dataset_to_tokenize.map(preprocess_data, batched=True)\n",
    "        tokenized_dataset.save_to_disk(ENCODED_DATASET_PATH)\n",
    "    else:\n",
    "        print(\"Loading tokenized dataset from cache\")\n",
    "        tokenized_dataset = datasets.load_from_disk(ENCODED_DATASET_PATH)\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "# ==============================================================#\n",
    "# ======================== MAIN PROGRAM ========================#\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "full_dataset = load_data()\n",
    "print(\"-\"*20, \"Dataset Loaded \")\n",
    "\n",
    "tokenized_dataset = tokenize_dataset(full_dataset)\n",
    "print(\"-\"*20, \"Dataset Tokenized \")\n",
    "print(\"TKNZD: \", tokenized_dataset)\n",
    "\n",
    "sys.exit(1)\n",
    "\n",
    "training_args = get_training_args()\n",
    "print(\"-\"*20, \"Training Args Ready \")\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "print(\"-\"*20, \"Rouge Metric Ready \")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "\n",
    "trainer = get_trainer(tokenized_dataset)\n",
    "print(\"-\"*20, \"Trainer Ready \")\n",
    "\n",
    "# Start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_dir}'/runs\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ Utility Methods ===============================#\n",
    "# ================================================================================#\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelWithLMHead, \n",
    "                          T5Tokenizer, T5ForConditionalGeneration, \n",
    "                          GPT2Tokenizer, GPT2LMHeadModel, \n",
    "                          OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
    "MODEL_CLASSES = {\n",
    "    \"mrm8488/t5-base-finetuned-wikiSQL\": (AutoModelWithLMHead, AutoTokenizer),\n",
    "    \"anusha/t5-base-finetuned-wikiSQL-sql-to-en_15i\": (AutoModelWithLMHead, AutoTokenizer),\n",
    "    \"t5-small\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "def get_model(model_args):\n",
    "    # Initialize the model and tokenizer\n",
    "    try:\n",
    "        model_class, tokenizer_class = MODEL_CLASSES[ model_args[\"model_name\"] ]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained( model_args[\"model_name\"] )\n",
    "    model = model_class.from_pretrained( model_args[\"model_name\"], pad_token_id=tokenizer.eos_token_id )\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def get_encoded_input(tokenizer, model_args, input_text):\n",
    "\n",
    "    # will have input_ids & attention_mask keys & corresponding tensors\n",
    "    encoded_input = tokenizer(\n",
    "        input_text,\n",
    "        padding=True, \n",
    "        max_length = model_args[\"max_target_length\"],\n",
    "        truncation=True, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors='pt')\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def get_encoded_output(model, model_args, encoded_input):\n",
    "\n",
    "    # URL: https://huggingface.co/blog/how-to-generate\n",
    "    output = model.generate(\n",
    "        input_ids = encoded_input['input_ids'],\n",
    "        attention_mask = encoded_input['attention_mask'], \n",
    "        temperature = model_args[\"temperature\"],\n",
    "        max_new_tokens=model_args[\"max_target_length\"],\n",
    "\n",
    "        # ----- Beam Search w/ return sequences -----#\n",
    "        #num_beams=5, \n",
    "        #no_repeat_ngram_size=2,\n",
    "        #early_stopping=True,\n",
    "        #num_return_sequences=5, #num_return_sequences<=num_beams\n",
    "        \n",
    "        # ----- Top P & Top K sampling -----#\n",
    "        #do_sample=True,\n",
    "        #top_k=top_k, \n",
    "        #top_p=top_p,\n",
    "        #num_return_sequences=3\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def get_input_text():\n",
    "    task_prefix = \"translate English to SQL: \"\n",
    "\n",
    "    query = \"What was North Melbourne's score as the home team?\"\n",
    "    query = \"Cars built after 2020 and manufactured in Italy\"\n",
    "    #query = \"which customers ordered in 1997 but did not order in 1998. Use cust_id in order table\"\n",
    "    #query = \"which customers in order table ordered in 1997 but did &* ^ # not order in 1998\"\n",
    "    #query = \"which customers (order table) ordered in 1997 (order_data column) but did not order in 1998\"\n",
    "\n",
    "    #input_text = \"translate English to SQL: %s </s>\" % query\n",
    "    #input_text = \"translate English to SQL: %s\" % query\n",
    "    input_text = f\"<pad>{task_prefix} {query}</s>\"\n",
    "    \n",
    "    print(\"INPUT: \", input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Using Pretrained or RetrainedModel ======================#\n",
    "# ================================================================================#\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model_name = \"openai-gpt\"\n",
    "model_name =\"t5-small\" # https://huggingface.co/t5-base\n",
    "model_name = \"mrm8488/t5-base-finetuned-wikiSQL\"\n",
    "\n",
    "model_args = {\n",
    "    \"model_name\": model_name,\n",
    "    \"temperature\": 0.95,\n",
    "    \"max_input_length\": 512,\n",
    "    \"max_target_length\": 64,\n",
    "}\n",
    "\n",
    "# Data Loading & Preprocesing\n",
    "# ---------------------------\n",
    "input_text = get_input_text()\n",
    "\n",
    "\n",
    "# Get Tokenizer & Model\n",
    "# ---------------------------\n",
    "tokenizer, model = get_model(model_args)\n",
    "\n",
    "\n",
    "# Input Data Tokenization\n",
    "# ---------------------------\n",
    "encoded_input = get_encoded_input(tokenizer, model_args, input_text)\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "# ---------------------------\n",
    "output = get_encoded_output(model, model_args, encoded_input)\n",
    "\n",
    "\n",
    "# Model Training\n",
    "# ---------------------------\n",
    "# for training, we can provide labels to tokenize\n",
    "\n",
    "\n",
    "# Traslation output Prediction\n",
    "# ---------------------------\n",
    "result = tokenizer.decode(output[0], skip_special_tokens = True, clean_up_tokenization_spaces=True)\n",
    "print(\"OUTPUT: \", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
