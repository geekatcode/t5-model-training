{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ Utility Methods ===============================#\n",
    "# ================================================================================#\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelWithLMHead, \n",
    "                          T5Tokenizer, T5ForConditionalGeneration, \n",
    "                          GPT2Tokenizer, GPT2LMHeadModel, \n",
    "                          OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
    "MODEL_CLASSES = {\n",
    "    \"mrm8488/t5-base-finetuned-wikiSQL\": (AutoModelWithLMHead, AutoTokenizer),\n",
    "    \"anusha/t5-base-finetuned-wikiSQL-sql-to-en_15i\": (AutoModelWithLMHead, AutoTokenizer),\n",
    "    \"t5-small\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "def get_model(model_args):\n",
    "    # Initialize the model and tokenizer\n",
    "    try:\n",
    "        model_class, tokenizer_class = MODEL_CLASSES[ model_args[\"model_name\"] ]\n",
    "    except KeyError:\n",
    "        raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained( model_args[\"model_name\"] )\n",
    "    model = model_class.from_pretrained( model_args[\"model_name\"], pad_token_id=tokenizer.eos_token_id )\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def get_encoded_input(tokenizer, model_args, input_text):\n",
    "\n",
    "    # will have input_ids & attention_mask keys & corresponding tensors\n",
    "    encoded_input = tokenizer(\n",
    "        input_text,\n",
    "        padding=True, \n",
    "        max_length = model_args[\"max_target_length\"],\n",
    "        truncation=True, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors='pt')\n",
    "    return encoded_input\n",
    "\n",
    "\n",
    "def get_encoded_output(model, model_args, encoded_input):\n",
    "\n",
    "    # URL: https://huggingface.co/blog/how-to-generate\n",
    "    output = model.generate(\n",
    "        input_ids = encoded_input['input_ids'],\n",
    "        attention_mask = encoded_input['attention_mask'], \n",
    "        temperature = model_args[\"temperature\"],\n",
    "        max_new_tokens=model_args[\"max_target_length\"],\n",
    "\n",
    "        # ----- Beam Search w/ return sequences -----#\n",
    "        #num_beams=5, \n",
    "        #no_repeat_ngram_size=2,\n",
    "        #early_stopping=True,\n",
    "        #num_return_sequences=5, #num_return_sequences<=num_beams\n",
    "        \n",
    "        # ----- Top P & Top K sampling -----#\n",
    "        #do_sample=True,\n",
    "        #top_k=top_k, \n",
    "        #top_p=top_p,\n",
    "        #num_return_sequences=3\n",
    "    )\n",
    "    return output\n",
    "\n",
    "def get_input_text():\n",
    "    task_prefix = \"translate English to SQL: \"\n",
    "\n",
    "    query = \"What was North Melbourne's score as the home team?\"\n",
    "    query = \"Cars built after 2020 and manufactured in Italy\"\n",
    "    #query = \"which customers ordered in 1997 but did not order in 1998. Use cust_id in order table\"\n",
    "    #query = \"which customers in order table ordered in 1997 but did &* ^ # not order in 1998\"\n",
    "    #query = \"which customers (order table) ordered in 1997 (order_data column) but did not order in 1998\"\n",
    "\n",
    "    #input_text = \"translate English to SQL: %s </s>\" % query\n",
    "    #input_text = \"translate English to SQL: %s\" % query\n",
    "    input_text = f\"<pad>{task_prefix} {query}</s>\"\n",
    "    \n",
    "    print(\"INPUT: \", input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Using Pretrained or RetrainedModel ======================#\n",
    "# ================================================================================#\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model_name = \"openai-gpt\"\n",
    "model_name =\"t5-small\" # https://huggingface.co/t5-base\n",
    "model_name = \"mrm8488/t5-base-finetuned-wikiSQL\"\n",
    "\n",
    "model_args = {\n",
    "    \"model_name\": model_name,\n",
    "    \"temperature\": 0.95,\n",
    "    \"max_input_length\": 512,\n",
    "    \"max_target_length\": 64,\n",
    "}\n",
    "\n",
    "# Data Loading & Preprocesing\n",
    "# ---------------------------\n",
    "input_text = get_input_text()\n",
    "\n",
    "\n",
    "# Get Tokenizer & Model\n",
    "# ---------------------------\n",
    "tokenizer, model = get_model(model_args)\n",
    "\n",
    "\n",
    "# Input Data Tokenization\n",
    "# ---------------------------\n",
    "encoded_input = get_encoded_input(tokenizer, model_args, input_text)\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "# ---------------------------\n",
    "output = get_encoded_output(model, model_args, encoded_input)\n",
    "\n",
    "\n",
    "# Model Training\n",
    "# ---------------------------\n",
    "# for training, we can provide labels to tokenize\n",
    "\n",
    "\n",
    "# Traslation output Prediction\n",
    "# ---------------------------\n",
    "result = tokenizer.decode(output[0], skip_special_tokens = True, clean_up_tokenization_spaces=True)\n",
    "print(\"OUTPUT: \", result)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
