{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Traing base T5 ==================== # \n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#================== Global Constants ==================#\n",
    "TASK_PREFIX     = {\n",
    "                    \"translation\":\"Translate English to SQL: \",\n",
    "                    \"metadata\":\"Find data tables in question: \"\n",
    "                    }\n",
    "TASK_KEY        = {\n",
    "                    \"translation\":\"sql\",\n",
    "                    \"metadata\":\"tables\"\n",
    "                    }\n",
    "TASK_HINT       = {\n",
    "                    \"translation\":\". Use following data tables - \",\n",
    "                    \"metadata\":\"\"\n",
    "                    }\n",
    "MODEL_CLASSES   = {\n",
    "                    \"t5-small\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "                    \"t5-base\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "                    }\n",
    "OPTIM_CLASSES   = {\n",
    "                    \"sgd\": optim.SGD,\n",
    "                    \"adam\": optim.Adam,\n",
    "                    }\n",
    "MODEL_BASE_DIR  = \"/Users/sree/.cache/huggingface/hub\"\n",
    "#LOG_DIR        = OUTPUT_DIR + \"/logs\"\n",
    "\n",
    "\n",
    "class MyT5Trainer:\n",
    "\n",
    "    #================== Training Logic Properties ==================#\n",
    "    task_mode           = \"translation\"\n",
    "    loss_log            = []\n",
    "\n",
    "    #================== Adam Hyperparameters ==================#\n",
    "    adam_lr             = 3e-4\n",
    "    adam_eps            = 1e-8\n",
    "    \n",
    "\n",
    "    def __init__(self, model_name, seed):\n",
    "        print(\"Initializing Trainer\")\n",
    "        self.__initialize_seed__(seed)\n",
    "\n",
    "        #init - self.model_name, self.out_model_name, self.output_dir\n",
    "        self.__initialize_model_dir__(model_name)\n",
    "\n",
    "        #init - self.tokenizer, self.model\n",
    "        self.__initialize_model__()\n",
    "\n",
    "        #init - self.optimizer\n",
    "        self.__initialize_optimizer__()\n",
    "        \n",
    "    def __initialize_seed__(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def __initialize_model_dir__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.out_model_name = f\"my--{self.model_name}--finetuned-text-to-SQL\"\n",
    "        self.output_dir = f\"{MODEL_BASE_DIR}/models--{self.out_model_name}\"\n",
    "    \n",
    "    def __initialize_model__(self):\n",
    "        try:\n",
    "            model_class, tokenizer_class = MODEL_CLASSES[ self.model_name ]\n",
    "        except KeyError:\n",
    "            raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "        # model_max_length=512,\n",
    "        self.tokenizer = tokenizer_class.from_pretrained( self.model_name )\n",
    "        self.model = model_class.from_pretrained(self.model_name, pad_token_id=self.tokenizer.eos_token_id )\n",
    "    \n",
    "    def __initialize_optimizer_param__(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,},\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,},]\n",
    "        return grouped_parameters\n",
    "    \n",
    "    def __initialize_optimizer__(self):\n",
    "        grouped_parameters = self.__initialize_optimizer_param__()\n",
    "        self.optimizer = optim.Adam(grouped_parameters, lr=self.adam_lr, eps=self.adam_eps)\n",
    "        #self.optimizer = optim.Adam(self.model.parameters*(), lr=self.ADAM_LR, eps=self.ADAM_EPS, weight_decay=0.0)\n",
    "\n",
    "    __tokenize__ = lambda self, text: self.tokenizer.encode_plus(\n",
    "        text, max_length=96, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    def __process_data__(self, record):\n",
    "        if self.skip_record(record):\n",
    "            return\n",
    "        \n",
    "        input = self.extract_input_from_ds(self.task_mode, record)\n",
    "        tokenized_input = self.__tokenize__(input)\n",
    "        input_ids  = tokenized_input[\"input_ids\"]\n",
    "        attention_mask = tokenized_input[\"attention_mask\"]\n",
    "        \n",
    "        expected_output = self.extract_expected_output_from_ds(self.task_mode, record)\n",
    "        tokenized_output = self.__tokenize__(expected_output)\n",
    "        lm_labels = tokenized_output[\"input_ids\"]\n",
    "        decoder_attention_mask=  tokenized_output[\"attention_mask\"]\n",
    "\n",
    "        # forward pass - predict\n",
    "        output = self.model(\n",
    "            input_ids = input_ids, attention_mask = attention_mask, \n",
    "            labels = lm_labels, decoder_attention_mask = decoder_attention_mask)\n",
    "        \n",
    "        # foward pass - compute loss\n",
    "        loss = output[0]\n",
    "        \n",
    "        #record the loss for plotting\n",
    "        self.loss_log.append(loss.item())\n",
    "\n",
    "        #zero all gradients before tha backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def set_data_extration_fuctions(self, extract_input, extract_expected_output, skip_record):\n",
    "        self.extract_input_from_ds = extract_input\n",
    "        self.extract_expected_output_from_ds = extract_expected_output\n",
    "        self.skip_record = skip_record\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_log, label = \"Stochastic Gradient Descent\")\n",
    "        #plt.plot(loss_Adam,label = \"Adam Optimizer\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('Cost/ total loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def train_model(self, epochs, ds, task_mode):\n",
    "        if self.extract_input_from_ds == None:\n",
    "            print(\"Set data extraction logic before training model\")\n",
    "            return\n",
    "        \n",
    "        self.loss_log = []\n",
    "        self.TASK_MODE = task_mode\n",
    "        print(\"Training \", self.TASK_MODE)\n",
    "        self.model.train(mode=True)\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch \",epoch)\n",
    "            ds.map(self.__process_data__)\n",
    "\n",
    "\n",
    "#================== Main Program ==================#\n",
    "\n",
    "#extract_input = lambda task_mode, record: TASK_PREFIX[task_mode] + record['question'] # No Hint\n",
    "extract_input = lambda task_mode, record: TASK_PREFIX[task_mode] + record['question'] + TASK_HINT[task_mode] + record['tables']\n",
    "extract_expected_output = lambda task_mode, record: record[ TASK_KEY[task_mode] ]\n",
    "skip_record = lambda record: record['comment']\n",
    "\n",
    "data_path = \"./data/my_flat_sql_data_meta.json\"\n",
    "my_ds = load_dataset('json', data_files = data_path)\n",
    "print(my_ds)\n",
    "\n",
    "# initialize tokenizer/encoder, model & optimizer\n",
    "trainer = MyT5Trainer(\"t5-small\", 42)\n",
    "trainer.set_data_extration_fuctions(extract_input, extract_expected_output, skip_record)\n",
    "\n",
    "task_modes = ['translation'] #'metadata'\n",
    "for task_mode in task_modes:\n",
    "    trainer.train_model(8, my_ds, task_mode)\n",
    "    #plot_loss(loss_log)\n",
    "    print(trainer.loss_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
