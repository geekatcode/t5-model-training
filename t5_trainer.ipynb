{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Traing base T5 ==================== # \n",
    "import sys, time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#================== Global Constants ==================#\n",
    "TASK_PREFIX     = {\n",
    "                    \"translation\":\"Translate English to SQL: \",\n",
    "                    \"metadata\":\"Find data tables in question: \"\n",
    "                    }\n",
    "TASK_KEY        = {\n",
    "                    \"translation\":\"sql\",\n",
    "                    \"metadata\":\"tables\"\n",
    "                    }\n",
    "TASK_HINT       = {\n",
    "                    \"translation\":\". Use following data tables - \",\n",
    "                    \"metadata\":\"\"\n",
    "                    }\n",
    "MODEL_CLASSES   = {\n",
    "                    \"t5-small\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "                    \"t5-base\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "                    }\n",
    "OPTIM_CLASSES   = {\n",
    "                    \"sgd\": optim.SGD,\n",
    "                    \"adam\": optim.Adam,\n",
    "                    }\n",
    "MODEL_BASE_DIR  = \"/Users/sree/.cache/huggingface/hub\"\n",
    "#LOG_DIR        = OUTPUT_DIR + \"/logs\"\n",
    "\n",
    "\n",
    "class MyT5Trainer:\n",
    "\n",
    "    #================== Training Logic Properties ==================#\n",
    "    train_mode          = \"record\"      #record|batch\n",
    "    task_mode           = \"translation\" #translation|metadata\n",
    "    loss_log            = []\n",
    "\n",
    "    #================== Adam Hyperparameters ==================#\n",
    "    adam_lr             = 3e-4\n",
    "    adam_eps            = 1e-8\n",
    "    \n",
    "\n",
    "    def __init__(self, model_name, seed):\n",
    "        print(\"Initializing Trainer\")\n",
    "        self.__initialize_seed__(seed)\n",
    "\n",
    "        #init - self.model_name, self.out_model_name, self.output_dir\n",
    "        self.__initialize_model_dir__(model_name)\n",
    "\n",
    "        #init - self.tokenizer, self.model\n",
    "        self.__initialize_model__()\n",
    "\n",
    "        #init - self.optimizer\n",
    "        self.__initialize_optimizer__()\n",
    "\n",
    "        #following to be setup before training:\n",
    "        # mode, train_mode, task_mode, extract_input_from_ds, extract_expected_output_from_ds, skip_record\n",
    "        \n",
    "    def __initialize_seed__(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def __initialize_model_dir__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.out_model_name = f\"my--{self.model_name}--finetuned-text-to-SQL\"\n",
    "        self.output_dir = f\"{MODEL_BASE_DIR}/models--{self.out_model_name}\"\n",
    "    \n",
    "    def __initialize_model__(self):\n",
    "        try:\n",
    "            model_class, tokenizer_class = MODEL_CLASSES[ self.model_name ]\n",
    "        except KeyError:\n",
    "            raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "        # model_max_length=512,\n",
    "        self.tokenizer = tokenizer_class.from_pretrained( self.model_name )\n",
    "        self.model = model_class.from_pretrained(self.model_name, pad_token_id=self.tokenizer.eos_token_id )\n",
    "    \n",
    "    def __initialize_optimizer_param__(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,},\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,},]\n",
    "        return grouped_parameters\n",
    "    \n",
    "    def __initialize_optimizer__(self):\n",
    "        grouped_parameters = self.__initialize_optimizer_param__()\n",
    "        self.optimizer = optim.Adam(grouped_parameters, lr=self.adam_lr, eps=self.adam_eps)\n",
    "        #self.optimizer = optim.Adam(self.model.parameters*(), lr=self.ADAM_LR, eps=self.ADAM_EPS, weight_decay=0.0)\n",
    "\n",
    "    def set_data_extractors(self, extract_input, extract_expected_output, skip_record):\n",
    "        self.extract_input_from_ds = extract_input\n",
    "        self.extract_expected_output_from_ds = extract_expected_output\n",
    "        self.skip_record = skip_record\n",
    "\n",
    "    # task modes - translation | metadata\n",
    "    def set_task_mode(self, task_mode):\n",
    "        self.TASK_MODE = task_mode\n",
    "\n",
    "    # train modes - record | batch\n",
    "    def set_data_mode(self, data_mode):\n",
    "        self.data_mode = data_mode\n",
    "\n",
    "    # mode - train | eval\n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "\n",
    "    def reset_accuracy_log(self):\n",
    "        self.loss_log = []\n",
    "        self.correctness = 0\n",
    "\n",
    "    def __train__(self):\n",
    "        self.set_mode(\"train\")\n",
    "        self.model.train(mode=True)\n",
    "\n",
    "    def __eval__(self):\n",
    "        self.set_mode(\"eval\")\n",
    "        self.model.eval()\n",
    "        \n",
    "    __tokenize__ = lambda self, text: self.tokenizer.encode_plus(\n",
    "        text, max_length=96, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __encode_data__(self, data):\n",
    "\n",
    "        # preprocessing data - extracting from input data file, prefixing & cleaning up\n",
    "        input = self.extract_input_from_ds(self.task_mode, data)\n",
    "        expected_output = self.extract_expected_output_from_ds(self.task_mode, data)\n",
    "\n",
    "        # tokenizing input & exptected output data\n",
    "        tokenized_input = self.__tokenize__(input)\n",
    "        tokenized_output = self.__tokenize__(expected_output)\n",
    "\n",
    "        return (tokenized_input[\"input_ids\"], tokenized_input[\"attention_mask\"], \n",
    "        tokenized_output[\"input_ids\"], tokenized_output[\"attention_mask\"])\n",
    "\n",
    "    def __process_data__(self, data):\n",
    "\n",
    "        #if training in single record mode, check for empty or comment records\n",
    "        if self.data_mode == 'record' and self.skip_record(data):\n",
    "            return\n",
    "        \n",
    "        # parse, cleanse & tokenize input data record or bacth records (based on train_mode)\n",
    "        start = time.time()\n",
    "        input_ids, attention_mask, lm_labels, decoder_attention_mask = self.__encode_data__(data)\n",
    "        end = time.time()\n",
    "        print(\"Time (sec) to Encode: \", end-start)\n",
    "\n",
    "        # forward pass - predict\n",
    "        start = time.time()\n",
    "        output = self.model(\n",
    "            input_ids = input_ids, attention_mask = attention_mask, \n",
    "            labels = lm_labels, decoder_attention_mask = decoder_attention_mask)\n",
    "        end = time.time()\n",
    "        print(\"Time (sec) to Predict: \", end-start)\n",
    "\n",
    "        if(self.mode == 'train'):\n",
    "            # foward pass - compute loss\n",
    "            loss = output[0]\n",
    "            \n",
    "            #record the loss for plotting\n",
    "            self.loss_log.append(loss.item())\n",
    "\n",
    "            #zero all gradients before tha backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if(self.mode == 'eval'):\n",
    "            print(\"Next iteration - EVAL\")\n",
    "            # Get the index of the max log-probability.\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            self.correctness += pred.eq(lm_labels.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_log, label = \"Stochastic Gradient Descent\")\n",
    "        #plt.plot(loss_Adam,label = \"Adam Optimizer\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('Cost/ total loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def train_model(self, epochs, train_ds, eval_ds):\n",
    "        if self.extract_input_from_ds == None:\n",
    "            print(\"Set data extraction logic before training model\")\n",
    "            return\n",
    "        \n",
    "        trainer.reset_accuracy_log()\n",
    "        batched = True if self.data_mode =='batch' else False\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch \",epoch)\n",
    "\n",
    "            # Model training\n",
    "            self.__train__()\n",
    "            train_ds.map(self.__process_data__, batched=batched)\n",
    "\n",
    "            # Model validation\n",
    "            # self.__eval__()\n",
    "            # with torch.no_grad():\n",
    "            #    eval_ds.map(self.__process_data__, batched=batched)\n",
    "            # accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "\n",
    "def compile_data_extractors(train_mode):\n",
    "    if train_mode == 'batch':\n",
    "        input_with_hint = lambda task_mode, tuple: TASK_PREFIX[task_mode] + tuple[0]+ TASK_HINT[task_mode] + tuple[1]\n",
    "        given_input = lambda task_mode, batch: [input_with_hint(task_mode, row) for row in zip(batch['question'], batch['tables']) if all(row)]\n",
    "        expected_output = lambda task_mode, batch: [row for row in batch[TASK_KEY[task_mode]] if row]\n",
    "        skip_record = None\n",
    "    else:\n",
    "        extract_input = lambda task_mode, record: TASK_PREFIX[task_mode] + record['question']\n",
    "        exract_hint = lambda task_mode, record: TASK_HINT[task_mode] + record['tables'] \n",
    "        given_input = lambda task_mode, record: extract_input(task_mode, record) + exract_hint(task_mode, record)\n",
    "        expected_output = lambda task_mode, record: record[ TASK_KEY[task_mode] ]\n",
    "        skip_record = lambda record: record['comment']\n",
    "    \n",
    "    return (given_input, expected_output, skip_record)\n",
    "\n",
    "def load_data():\n",
    "    data_path = \"./data/my_flat_sql_data_meta.json\"\n",
    "    train_ds = load_dataset('json', data_files = data_path)\n",
    "    #print(train_ds)\n",
    "\n",
    "    # will have to load eval ds separate\n",
    "    return train_ds, train_ds\n",
    "\n",
    "#================== Main Program ==================#\n",
    "\n",
    "# load training & evaluation/validation datasets\n",
    "train_ds,eval_ds = load_data()\n",
    "\n",
    "# record = one record at a time training \n",
    "# batch = training 1K batched records at a time - not yeilding expected results - needs investigation\n",
    "data_mode = \"record\" #record|batch\n",
    "\n",
    "# get lambdas for data extration\n",
    "given_input, expected_output, skip_record = compile_data_extractors(data_mode)\n",
    "\n",
    "# initialize tokenizer/encoder, model & optimizer\n",
    "trainer = MyT5Trainer(\"t5-base\", 42) # t5-base | t5-small\n",
    "\n",
    "# set the lamdas, these lambdas will get invoked automatically during data extraction\n",
    "trainer.set_data_extractors(given_input, expected_output, skip_record)\n",
    "\n",
    "epoch = 1\n",
    "task_modes = ['translation'] #'metadata'\n",
    "for task_mode in task_modes:\n",
    "    trainer.set_data_mode(data_mode)\n",
    "    trainer.set_task_mode(task_mode)\n",
    "\n",
    "    trainer.train_model(epoch, train_ds, eval_ds)\n",
    "    trainer.plot_loss()\n",
    "    print(trainer.loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_MODE = 'translation'\n",
    "input_text = TASK_PREFIX[TASK_MODE] + \"Which teams played in 2022?\" #+  \"</s>\"\n",
    "hint_text = TASK_HINT[TASK_MODE] + \"game_stats\"\n",
    "expected_output = \"SELECT team_name FROM game_stats WHERE DATE_PART('YEAR', pay_date)= 2022\"\n",
    "print(input_text+hint_text)\n",
    "\n",
    "trainer.set_data_mode('record')\n",
    "trainer.set_task_mode('translation')\n",
    "trainer.set_mode('eval')\n",
    "\n",
    "test_tokenized = trainer.tokenizer.encode_plus(input_text+hint_text, return_tensors=\"pt\")\n",
    "test_input_ids  = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "\n",
    "output_tokenized = trainer.tokenizer.encode_plus(expected_output, return_tensors=\"pt\")\n",
    "labels = output_tokenized[\"input_ids\"]\n",
    "\n",
    "trainer.model.eval()\n",
    "outputs = trainer.model.generate(\n",
    "    input_ids=test_input_ids,\n",
    "    attention_mask=test_attention_mask,\n",
    "    temperature = .96,\n",
    "    max_new_tokens=64,\n",
    "    #max_length=64,\n",
    "    \n",
    "    #early_stopping=True,\n",
    "    #num_beams=10,\n",
    "    #num_return_sequences=1, #3\n",
    "    #no_repeat_ngram_size=2 #2\n",
    "    \n",
    "    # ----- Beam Search w/ return sequences -----#\n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5, #num_return_sequences<=num_beams\n",
    "\n",
    "    # ----- Top P & Top K sampling -----# ANALYSIS - much faster than beam search\n",
    "    #do_sample=True,\n",
    "    #top_k=5, \n",
    "    #top_p=3,\n",
    "    #num_return_sequences=1\n",
    "\n",
    "    # --- Greedy search ----#\n",
    "    \n",
    ")\n",
    "\n",
    "for beam_output in outputs:\n",
    "    \n",
    "    output = trainer.tokenizer.decode(beam_output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(\"BEAM: \", beam_output)\n",
    "    print(output)\n",
    "\n",
    "    # Get the index of the max log-probability.\n",
    "    #pred = beam_output.argmax(dim=0, keepdim=True)\n",
    "    pred = torch.argmax(beam_output, dim=0, keepdim=True)\n",
    "    print(\"PRED Index: \",pred)\n",
    "\n",
    "    max = torch.max()\n",
    "    print(\"Max: \",max)\n",
    "\n",
    "    print(\"EXPected: \",expected_output)\n",
    "    print(\"LABELS: \",labels)\n",
    "\n",
    "    print(\"EQ: \", beam_output.eq(labels))\n",
    "\n",
    "    #print(\"EQ: \", labels.eq(pred).sum() )\n",
    "    #print(\"EQ: \", labels.eq(pred).sum().item() )\n",
    "\n",
    "    #print(\"VIEW SUM: \", labels.sum())\n",
    "    \n",
    "    #print(\"PRED SHAPE: \", pred.shape)\n",
    "    print(\"LABEL SHAPE: \", labels.shape)\n",
    "    \n",
    "    #print(\"VIEW AS: \",pred.view_as(labels))\n",
    "    #print(\"VIEW AS: \",labels.view_as(pred))\n",
    "\n",
    "    #correct = pred.eq(labels.view_as(pred)).sum()\n",
    "    correct = pred.eq(labels).sum().item()\n",
    "    print(\"CORRECT: \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract_input = lambda task_mode, record: TASK_PREFIX[task_mode] + record['question']\n",
    "#exract_hint = lambda task_mode, record: TASK_HINT[task_mode] + record['tables'] \n",
    "#extract_input_with_hint = lambda task_mode, record: extract_input(task_mode, record) + exract_hint(task_mode, record)\n",
    "#expected_output = lambda task_mode, record: record[ TASK_KEY[task_mode] ]\n",
    "#skip_record = lambda record: record['comment']\n",
    "\n",
    "#join_input_hint = lambda task_mode, tuple: TASK_PREFIX[task_mode] + tuple[0]+ TASK_HINT[task_mode] + tuple[1]\n",
    "#input_batch = lambda task_mode, batch: [join_input_hint(task_mode, row) for row in zip(batch['question'], batch['tables']) if all(row)]\n",
    "#expected_output_batch = lambda task_mode, batch: [row for row in batch[TASK_KEY[task_mode]] if row]\n",
    "#skip_record_batch = None\n",
    "\n",
    "# one record at a time training \n",
    "#trainer.set_data_extractors(train_mode, given_input, expected_output, skip_record)\n",
    "\n",
    "# batch training is not yeilding expected results - need to investigate\n",
    "#trainer.set_data_extractors(train_mode, given_input, expected_output, skip_record)\n",
    "\n",
    "\n",
    "    #if(self.train_mode =='record'):\n",
    "        #    ds.map(self.__process_data__, batched=False)\n",
    "        #if (self.train_mode =='batch'):\n",
    "        #    ds.map(self.__process_data__, batched=True)\n",
    "\n",
    "\n",
    "        #input_ids  = tokenized_input[\"input_ids\"]\n",
    "        #attention_mask = tokenized_input[\"attention_mask\"]\n",
    "        #lm_labels = tokenized_output[\"input_ids\"]\n",
    "        #decoder_attention_mask=  tokenized_output[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor(1,2)\n",
    "t2 = torch.Tensor([20])\n",
    "t3 = torch.Tensor([[2,20,3,4,5]])\n",
    "#t4 = torch.Tensor([],[])\n",
    "\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)\n",
    "print(t3.size(0))\n",
    "print(t3.size(1))\n",
    "#print(\"VIEW:\", t3.view(-1,-1))\n",
    "eqt = t3.eq(t2)\n",
    "print(\"EQ: \", eqt )\n",
    "print(\"SUM: \", eqt.sum() )\n",
    "print(\"ITEM: \", eqt.sum().item() )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
