{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Module Imports ==================== # \n",
    "import sys, time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#================== Global Constants ==================#\n",
    "TASK_PREFIX     = {\n",
    "                    \"translation\":\"Translate English to SQL: \",\n",
    "                    \"metadata\":\"Find data tables in question: \"\n",
    "                    }\n",
    "TASK_KEY        = {\n",
    "                    \"translation\":\"sql\",\n",
    "                    \"metadata\":\"tables\"\n",
    "                    }\n",
    "TASK_HINT       = {\n",
    "                    \"translation\":\". Use following data tables - \",\n",
    "                    \"metadata\":\"\"\n",
    "                    }\n",
    "MODEL_CLASSES   = {\n",
    "                    \"t5-small\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "                    \"t5-base\": (T5ForConditionalGeneration, T5Tokenizer),\n",
    "                    }\n",
    "OPTIM_CLASSES   = {\n",
    "                    \"sgd\": optim.SGD,\n",
    "                    \"adam\": optim.Adam,\n",
    "                    }\n",
    "MODEL_BASE_DIR  = \"/Users/sree/.cache/huggingface/hub\"\n",
    "#LOG_DIR        = OUTPUT_DIR + \"/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataLoader:\n",
    "    \n",
    "    data_mode       = \"record\" #record|batch\n",
    "    task_mode       = \"translation\" #translation|metadata\n",
    "    given_input     = None\n",
    "    expected_output = None\n",
    "    skip_record     = None\n",
    "\n",
    "    def __init__(self, data_mode):\n",
    "        self.set_data_mode(data_mode)\n",
    "        self.__init_data_extractors__()\n",
    "\n",
    "    # train modes - record | batch\n",
    "    def set_data_mode(self, data_mode):\n",
    "        self.data_mode = data_mode\n",
    "\n",
    "    # task modes - translation | metadata\n",
    "    def set_task_mode(self, task_mode):\n",
    "        self.task_mode = task_mode\n",
    "\n",
    "    def load_data(self):\n",
    "        data_path = \"./data/my_flat_sql_data_meta.json\"\n",
    "        train_ds = load_dataset('json', data_files = data_path)\n",
    "        #print(train_ds)\n",
    "\n",
    "        # will have to load eval ds separate\n",
    "        return train_ds, train_ds\n",
    "    \n",
    "    def __init_data_extractors__(self):\n",
    "        if self.data_mode == 'batch':\n",
    "            input_with_hint = lambda task_mode, tuple: TASK_PREFIX[task_mode] + tuple[0]+ TASK_HINT[task_mode] + tuple[1]\n",
    "\n",
    "            self.given_input = lambda task_mode, batch: [input_with_hint(task_mode, row) for row in zip(batch['question'], batch['tables']) if all(row)]\n",
    "            self.expected_output = lambda task_mode, batch: [row for row in batch[TASK_KEY[task_mode]] if row]\n",
    "            self.skip_record = None\n",
    "        if self.data_mode == 'record':\n",
    "            extract_input = lambda task_mode, record: TASK_PREFIX[task_mode] + record['question']\n",
    "            exract_hint = lambda task_mode, record: TASK_HINT[task_mode] + record['tables']\n",
    "\n",
    "            self.given_input = lambda task_mode, record: extract_input(task_mode, record) + exract_hint(task_mode, record)\n",
    "            self.expected_output = lambda task_mode, record: record[ TASK_KEY[task_mode] ]\n",
    "            self.skip_record = lambda record: record['comment']\n",
    "    \n",
    "    def exract_input(self, data):\n",
    "        return self.given_input(self.task_mode, data)\n",
    "\n",
    "    def extract_expected_output(self, data):\n",
    "        return self.expected_output(self.task_mode, data)\n",
    "    \n",
    "    def is_skip_record(self, data):\n",
    "        return self.data_mode == 'record' and self.skip_record(data)\n",
    "    \n",
    "    def is_read_bacthed(self):\n",
    "        return self.data_mode == 'batch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3a7807e761e3bdc4\n",
      "Found cached dataset json (/Users/sree/.cache/huggingface/datasets/json/default-3a7807e761e3bdc4/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92712f33841c4dba94e355b8241a723c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sree/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd06f5556e764ba49133ab43c161aa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "map() got an unexpected keyword argument 'batched'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 201\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m task_mode \u001b[39min\u001b[39;00m task_modes:\n\u001b[1;32m    200\u001b[0m     dataLoader\u001b[39m.\u001b[39mset_task_mode(task_mode)\n\u001b[0;32m--> 201\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain_model(epoch, train_ds, eval_ds)\n\u001b[1;32m    203\u001b[0m     trainer\u001b[39m.\u001b[39mplot_loss()\n\u001b[1;32m    204\u001b[0m     \u001b[39mprint\u001b[39m(trainer\u001b[39m.\u001b[39mloss_log)\n",
      "Cell \u001b[0;32mIn[33], line 166\u001b[0m, in \u001b[0;36mMyT5Trainer.train_model\u001b[0;34m(self, epochs, train_ds, eval_ds)\u001b[0m\n\u001b[1;32m    163\u001b[0m train_ds\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__process_train_data__, batched\u001b[39m=\u001b[39mbatched)\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 166\u001b[0m     executor\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__process_train_data__, batched\u001b[39m=\u001b[39;49mbatched)\n",
      "\u001b[0;31mTypeError\u001b[0m: map() got an unexpected keyword argument 'batched'"
     ]
    }
   ],
   "source": [
    "class MyT5Trainer:\n",
    "    \n",
    "    #================== Contained Objects ==================#\n",
    "    # loader, tokenizer, model, optimizer\n",
    "    \n",
    "    #================== Model Info Attributes ==================#\n",
    "    #out_model_name      = None\n",
    "    #output_dir          = None\n",
    "\n",
    "    #================== Trainer State Control attributes ==================#\n",
    "    loss_log            = []\n",
    "\n",
    "    #================== T5 Model Hyperparameters ==================#\n",
    "    #\n",
    "    \n",
    "    #================== Adam Optimizer Hyperparameters ==================#\n",
    "    adam_lr             = 3e-4\n",
    "    adam_eps            = 1e-8\n",
    "    \n",
    "\n",
    "    def __init__(self, model_name, seed, loader):\n",
    "        \n",
    "        print(\"Initializing Trainer\")\n",
    "        self.__init_seed__(seed)\n",
    "        self.set_loader(loader)\n",
    "\n",
    "        #init - self.tokenizer, self.model\n",
    "        self.__init_model_and_encoder__(model_name)\n",
    "\n",
    "        #init - self.optimizer\n",
    "        self.__init_optimizer__()\n",
    "    \n",
    "    #================== Initializer Functions ==================#\n",
    "    #===========================================================#\n",
    "\n",
    "    def __init_seed__(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "    def __init_model_and_encoder__(self, model_name):\n",
    "        \n",
    "        # will be enabled in future when saving model checkpointing\n",
    "        #self.out_model_name = f\"my--{model_name}--finetuned-text-to-SQL\"\n",
    "        #self.output_dir = f\"{MODEL_BASE_DIR}/models--{self.out_model_name}\"\n",
    "\n",
    "        try:\n",
    "            model_class, tokenizer_class = MODEL_CLASSES[ model_name ]\n",
    "        except KeyError:\n",
    "            raise KeyError(\"the model {} you specified is not supported. You are welcome to add it and open a PR :)\")\n",
    "\n",
    "        # model_max_length=512,\n",
    "        self.tokenizer = tokenizer_class.from_pretrained( model_name )\n",
    "        self.model = model_class.from_pretrained(model_name, pad_token_id=self.tokenizer.eos_token_id )\n",
    "\n",
    "\n",
    "    def __init_optimizer__(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        grouped_parameters = [\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,},\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,},]\n",
    "        self.optimizer = optim.Adam(grouped_parameters, lr=self.adam_lr, eps=self.adam_eps)\n",
    "        #self.optimizer = optim.Adam(self.model.parameters*(), lr=self.ADAM_LR, eps=self.ADAM_EPS, weight_decay=0.0)\n",
    "\n",
    "\n",
    "    def set_loader(self, oader):\n",
    "        self.loader = oader\n",
    "\n",
    "\n",
    "    def reset_accuracy_log(self):\n",
    "        self.loss_log = []\n",
    "        self.correctness = 0\n",
    "\n",
    "    #==================== Functional Methods ===================#\n",
    "    #===========================================================#\n",
    "        \n",
    "    __tokenize__ = lambda self, text: self.tokenizer.encode_plus(\n",
    "        text, max_length=96, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __encode_data__(self, data):\n",
    "\n",
    "        # preprocessing data - extracting from input data, prefixing & cleaning up\n",
    "        input = self.loader.exract_input(data) #self.task_mode,\n",
    "        expected_output = self.loader.extract_expected_output(data)\n",
    "\n",
    "        # tokenizing input & exptected output data\n",
    "        tokenized_input = self.__tokenize__(input)\n",
    "        tokenized_output = self.__tokenize__(expected_output)\n",
    "\n",
    "        return (tokenized_input[\"input_ids\"], tokenized_input[\"attention_mask\"], \n",
    "        tokenized_output[\"input_ids\"], tokenized_output[\"attention_mask\"])\n",
    "    \n",
    "\n",
    "    def __generate_prediction__(self, data):\n",
    "\n",
    "        #if training in single record mode, check for empty or comment records\n",
    "        if self.loader.is_skip_record(data):\n",
    "            return None\n",
    "        \n",
    "        # parse, cleanse & tokenize input data record or bacth records (based on train_mode)\n",
    "        input_ids, attention_mask, lm_labels, decoder_attention_mask = self.__encode_data__(data)\n",
    "\n",
    "        # forward pass - predict\n",
    "        return self.model(\n",
    "            input_ids = input_ids, attention_mask = attention_mask, \n",
    "            labels = lm_labels, decoder_attention_mask = decoder_attention_mask)\n",
    "    \n",
    "    \n",
    "    def __process_train_data__(self, data):\n",
    "\n",
    "        output = self.__generate_prediction__(data)\n",
    "        #in case of skip records\n",
    "        if output is None:\n",
    "            return\n",
    "        \n",
    "        # foward pass - compute loss\n",
    "        loss = output[0]\n",
    "        \n",
    "        #record the loss for plotting\n",
    "        self.loss_log.append(loss.item())\n",
    "\n",
    "        #zero all gradients before tha backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "    def __process_eval_data__(self, data):\n",
    "        \n",
    "        output = self.__generate_prediction__(data)\n",
    "        #in case of skip records\n",
    "        if output is None:\n",
    "            return\n",
    "\n",
    "        print(\"Next iteration - EVAL\")\n",
    "        # Get the index of the max log-probability.\n",
    "        #pred = output.argmax(dim=1, keepdim=True)\n",
    "        #self.correctness += pred.eq(lm_labels.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    def train_model(self, epochs, train_ds, eval_ds):\n",
    "        batched=self.loader.is_read_bacthed()\n",
    "        trainer.reset_accuracy_log()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch \",epoch)\n",
    "\n",
    "            # Model training\n",
    "            train_ds.map(self.__process_train_data__, batched=batched)\n",
    "\n",
    "            # Model validation\n",
    "            # with torch.no_grad():\n",
    "            #    eval_ds.map(self.__process_eval_data__, batched=batched)\n",
    "            # accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_log, label = \"Stochastic Gradient Descent\")\n",
    "        #plt.plot(loss_Adam,label = \"Adam Optimizer\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('Cost/ total loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#================== Main Program ==================#\n",
    "#==================================================#\n",
    "\n",
    "# record = one record at a time training \n",
    "# batch = training 1K batched records at a time - not yeilding expected results - needs investigation\n",
    "data_mode = \"record\" #record|batch\n",
    "dataLoader = MyDataLoader(data_mode)\n",
    "\n",
    "# load training & evaluation/validation datasets\n",
    "train_ds, eval_ds = dataLoader.load_data()\n",
    "\n",
    "# initialize tokenizer/encoder, model & optimizer\n",
    "trainer = MyT5Trainer(\"t5-base\", 42, dataLoader) # t5-base | t5-small\n",
    "\n",
    "epoch = 1\n",
    "task_modes = ['translation'] #'metadata'\n",
    "for task_mode in task_modes:\n",
    "    \n",
    "    dataLoader.set_task_mode(task_mode)\n",
    "    trainer.train_model(epoch, train_ds, eval_ds)\n",
    "\n",
    "    trainer.plot_loss()\n",
    "    print(trainer.loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_MODE = 'translation'\n",
    "input_text = TASK_PREFIX[TASK_MODE] + \"Which teams played in 2022?\" #+  \"</s>\"\n",
    "hint_text = TASK_HINT[TASK_MODE] + \"game_stats\"\n",
    "expected_output = \"SELECT team_name FROM game_stats WHERE DATE_PART('YEAR', pay_date)= 2022\"\n",
    "print(input_text+hint_text)\n",
    "\n",
    "trainer.set_data_mode('record')\n",
    "trainer.set_task_mode('translation')\n",
    "trainer.set_mode('eval')\n",
    "\n",
    "test_tokenized = trainer.tokenizer.encode_plus(input_text+hint_text, return_tensors=\"pt\")\n",
    "test_input_ids  = test_tokenized[\"input_ids\"]\n",
    "test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "\n",
    "output_tokenized = trainer.tokenizer.encode_plus(expected_output, return_tensors=\"pt\")\n",
    "labels = output_tokenized[\"input_ids\"]\n",
    "\n",
    "trainer.model.eval()\n",
    "outputs = trainer.model.generate(\n",
    "    input_ids=test_input_ids,\n",
    "    attention_mask=test_attention_mask,\n",
    "    temperature = .96,\n",
    "    max_new_tokens=64,\n",
    "    #max_length=64,\n",
    "    \n",
    "    #early_stopping=True,\n",
    "    #num_beams=10,\n",
    "    #num_return_sequences=1, #3\n",
    "    #no_repeat_ngram_size=2 #2\n",
    "    \n",
    "    # ----- Beam Search w/ return sequences -----#\n",
    "    early_stopping=True,\n",
    "    num_beams=10,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5, #num_return_sequences<=num_beams\n",
    "\n",
    "    # ----- Top P & Top K sampling -----# ANALYSIS - much faster than beam search\n",
    "    #do_sample=True,\n",
    "    #top_k=5, \n",
    "    #top_p=3,\n",
    "    #num_return_sequences=1\n",
    "\n",
    "    # --- Greedy search ----#\n",
    "    \n",
    ")\n",
    "\n",
    "for beam_output in outputs:\n",
    "    \n",
    "    output = trainer.tokenizer.decode(beam_output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(\"BEAM: \", beam_output)\n",
    "    print(output)\n",
    "\n",
    "    # Get the index of the max log-probability.\n",
    "    #pred = beam_output.argmax(dim=0, keepdim=True)\n",
    "    pred = torch.argmax(beam_output, dim=0, keepdim=True)\n",
    "    print(\"PRED Index: \",pred)\n",
    "\n",
    "    max = torch.max()\n",
    "    print(\"Max: \",max)\n",
    "\n",
    "    print(\"EXPected: \",expected_output)\n",
    "    print(\"LABELS: \",labels)\n",
    "\n",
    "    print(\"EQ: \", beam_output.eq(labels))\n",
    "\n",
    "    #print(\"EQ: \", labels.eq(pred).sum() )\n",
    "    #print(\"EQ: \", labels.eq(pred).sum().item() )\n",
    "\n",
    "    #print(\"VIEW SUM: \", labels.sum())\n",
    "    \n",
    "    #print(\"PRED SHAPE: \", pred.shape)\n",
    "    print(\"LABEL SHAPE: \", labels.shape)\n",
    "    \n",
    "    #print(\"VIEW AS: \",pred.view_as(labels))\n",
    "    #print(\"VIEW AS: \",labels.view_as(pred))\n",
    "\n",
    "    #correct = pred.eq(labels.view_as(pred)).sum()\n",
    "    correct = pred.eq(labels).sum().item()\n",
    "    print(\"CORRECT: \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor(1,2)\n",
    "t2 = torch.Tensor([20])\n",
    "t3 = torch.Tensor([[2,20,3,4,5]])\n",
    "#t4 = torch.Tensor([],[])\n",
    "\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "print(t3.shape)\n",
    "print(t3.size(0))\n",
    "print(t3.size(1))\n",
    "#print(\"VIEW:\", t3.view(-1,-1))\n",
    "eqt = t3.eq(t2)\n",
    "print(\"EQ: \", eqt )\n",
    "print(\"SUM: \", eqt.sum() )\n",
    "print(\"ITEM: \", eqt.sum().item() )\n",
    "\n",
    "\n",
    "# mode - train | eval\n",
    "#def set_mode(self, mode):\n",
    "#    self.mode = mode\n",
    "\n",
    "#def __train__(self):\n",
    "    #    self.set_mode(\"train\")\n",
    "    #    self.model.train(mode=True)\n",
    "\n",
    "    #def __eval__(self):\n",
    "    #    self.set_mode(\"eval\")\n",
    "    #    self.model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
